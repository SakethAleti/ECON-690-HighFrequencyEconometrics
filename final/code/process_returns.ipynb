{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.contrib import concurrent\n",
    "from multiprocessing import Pool\n",
    "import itertools\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_returns(data_df):\n",
    "    \n",
    "    # Prices\n",
    "    data_df['log_price'] = np.log(data_df['price'])\n",
    "    data_df['return'] = data_df.groupby(['ticker', 'date'])['log_price'].transform(pd.Series.diff)\n",
    "    \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_return_stats(data_df):\n",
    "\n",
    "    # Return vars\n",
    "    data_df[\"return_p2\"] = np.power(data_df[\"return\"], 2)\n",
    "    data_df[\"return_p4\"] = np.power(data_df[\"return\"], 4)\n",
    "    data_df[\"return_lag1\"] = data_df.groupby([\"date\"])[\"return\"].shift(1)\n",
    "    data_df[\"return_bp\"] = np.abs(np.multiply(data_df[\"return\"], data_df[\"return_lag1\"]))\n",
    "\n",
    "    # BV\n",
    "    data_df[\"bipower_var_daily\"] = data_df.groupby([\"date\"])[\"return_bp\"].transform(\"sum\") * (\n",
    "        78 / 77\n",
    "    )\n",
    "    data_df[\"bipower_var_tod\"] = data_df.groupby([\"time\"])[\"return_bp\"].transform(\"mean\")\n",
    "\n",
    "    # Jump detection\n",
    "    alpha = 4\n",
    "    data_df[\"tod_correction\"] = np.divide(\n",
    "        data_df[\"bipower_var_tod\"], data_df[\"bipower_var_tod\"].mean()\n",
    "    )\n",
    "    data_df[\"jump_cut\"] = (\n",
    "        alpha * np.sqrt(data_df[\"bipower_var_daily\"] * data_df[\"tod_correction\"]) * ((1 / 78) ** (0.49))\n",
    "    )\n",
    "    data_df['is_jump'] = (np.abs(data_df['return']) > data_df['jump_cut']).astype(int)\n",
    "\n",
    "    ## Filtered returns\n",
    "    # Realized Semivariance components\n",
    "    data_df['return_p2_pos'] = data_df['return_p2']*(data_df['return'] > 0)\n",
    "    data_df['return_p2_neg'] = data_df['return_p2']*(data_df['return'] < 0)\n",
    "    \n",
    "    # Signed jump variation\n",
    "    data_df['sgn_jump_var'] = data_df['return_p2_pos'] - data_df['return_p2_neg'] \n",
    "    \n",
    "    ## Filtered diffusive returns\n",
    "    # Main\n",
    "    data_df['return_d'] = data_df['return']*(1-data_df['is_jump'])\n",
    "    data_df['return_d_pos'] = data_df['return_d']*(data_df['return_d'] > 0)\n",
    "    data_df['return_d_neg'] = data_df['return_d']*(data_df['return_d'] < 0)\n",
    "    # Semi\n",
    "    data_df['return_d_p2'] = np.power(data_df['return_d'], 2)\n",
    "    data_df['return_d_p4'] = np.power(data_df['return_d'], 4)\n",
    "    data_df['return_d_p2_pos'] = np.power(data_df['return_d_pos'], 2)\n",
    "    data_df['return_d_p4_pos'] = np.power(data_df['return_d_pos'], 4)\n",
    "    data_df['return_d_p2_neg'] = np.power(data_df['return_d_neg'], 2)\n",
    "    data_df['return_d_p4_neg'] = np.power(data_df['return_d_neg'], 4)\n",
    "\n",
    "    ## Filtered jump returns\n",
    "    # Main\n",
    "    data_df['return_j'] = data_df['return']*(data_df['is_jump'])\n",
    "    data_df['return_j_pos'] = data_df['return_j']*(data_df['return_j'] > 0)\n",
    "    data_df['return_j_neg'] = data_df['return_j']*(data_df['return_j'] < 0)\n",
    "    # Semi\n",
    "    data_df['return_j_p2'] = np.power(data_df['return_j'], 2)\n",
    "    data_df['return_j_p4'] = np.power(data_df['return_j'], 4)\n",
    "    data_df['return_j_p2_pos'] = np.power(data_df['return_j_pos'], 2)\n",
    "    data_df['return_j_p4_pos'] = np.power(data_df['return_j_pos'], 4)\n",
    "    data_df['return_j_p2_neg'] = np.power(data_df['return_j_neg'], 2)\n",
    "    data_df['return_j_p4_neg'] = np.power(data_df['return_j_neg'], 4)\n",
    "    \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rv_stats(data_df):\n",
    "\n",
    "    ## Returns data\n",
    "    rv_df = (\n",
    "        data_df.drop([\"price\", \"log_price\"], axis=1)\n",
    "        .groupby([\"ticker\", \"date\"])\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    ## Realized Vol\n",
    "    rv_df.rename(\n",
    "        columns = {\n",
    "            \"return_p2\": \"rv\",\n",
    "            \"return_p2_pos\": \"rs_pos\",\n",
    "            \"return_p2_neg\": \"rs_neg\",\n",
    "            \"sgn_jump_var\": \"sgn_jump_var\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # Lags\n",
    "    rv_df[\"rv_lag_1\"] = rv_df[\"rv\"].shift(1)\n",
    "    rv_df[\"rv_lag_m5\"] = rv_df[\"rv_lag_1\"].rolling(5).mean()\n",
    "    rv_df[\"rv_lag_m22\"] = rv_df[\"rv_lag_1\"].rolling(22).mean()\n",
    "\n",
    "    rv_df[\"rv_d_1\"] = rv_df[\"return_d_p2\"]\n",
    "    rv_df[\"rv_d_5\"] = rv_df[\"rv_d_1\"].rolling(5).mean()\n",
    "    rv_df[\"rv_d_22\"] = rv_df[\"rv_d_1\"].rolling(22).mean()\n",
    "\n",
    "    # Total jumps\n",
    "    rv_df[\"sum_jump_p2_1\"] = rv_df[\"return_j_p2\"] * 100\n",
    "    rv_df[\"sum_jump_p2_5\"] = rv_df[\"sum_jump_p2_1\"].rolling(5).mean()\n",
    "    rv_df[\"sum_jump_p2_22\"] = rv_df[\"sum_jump_p2_1\"].rolling(22).mean()\n",
    "\n",
    "    rv_df[\"sum_jump_p2_pos_1\"] = rv_df[\"return_j_p2_pos\"] * 100\n",
    "    rv_df[\"sum_jump_p2_pos_5\"] = rv_df[\"sum_jump_p2_pos_1\"].rolling(5).mean()\n",
    "    rv_df[\"sum_jump_p2_pos_22\"] = rv_df[\"sum_jump_p2_pos_1\"].rolling(22).mean()\n",
    "\n",
    "    rv_df[\"sum_jump_p2_neg_1\"] = rv_df[\"return_j_p2_neg\"] * 100\n",
    "    rv_df[\"sum_jump_p2_neg_5\"] = rv_df[\"sum_jump_p2_neg_1\"].rolling(5).mean()\n",
    "    rv_df[\"sum_jump_p2_neg_22\"] = rv_df[\"sum_jump_p2_neg_1\"].rolling(22).mean()\n",
    "\n",
    "    return rv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Price Files\n",
    "Get returns and identify jumps in five minute prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ticker_file(filename):\n",
    "    \n",
    "    ticker = filename.split('/')[-1].split('_')[0]\n",
    "    raw_ticker_df = pd.read_feather(filename)\n",
    "    \n",
    "    return ticker, compute_return_stats(add_returns(raw_ticker_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7290c6e545cf47759ca86395c7e4b9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=455.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 18s, sys: 3min 56s, total: 5min 15s\n",
      "Wall time: 4min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_files = glob.glob(\"../data/prices/tickers_clean/*.feather\")\n",
    "\n",
    "with Pool(8) as p:\n",
    "\n",
    "    for ticker, processed_ticker_df in tqdm(\n",
    "        p.imap_unordered(process_ticker_file, data_files),\n",
    "        total=len(data_files),\n",
    "    ):\n",
    "\n",
    "        processed_ticker_df.to_feather(\n",
    "            f\"../data/returns/ticker_returns/{ticker}_processed.feather\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a50ad663f142e0a083732ed0424d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=455.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 6.72 s, sys: 2.48 s, total: 9.2 s\n",
      "Wall time: 9.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_files = glob.glob(\"../data/prices/tickers_clean/*.feather\")\n",
    "\n",
    "def temp_f(filename):\n",
    "    \n",
    "    ticker = filename.split('/')[-1].split('_')[0]\n",
    "    raw_ticker_df = pd.read_feather(filename)\n",
    "#     print(raw_ticker_df['ticker'].iloc[0], end = '  -- ')\n",
    "    temp = np.min(raw_ticker_df.groupby(['date'])['time'].count())\n",
    "    if temp < 78:\n",
    "        print(raw_ticker_df['ticker'].iloc[0])\n",
    "        print(temp)\n",
    "    \n",
    "    return None\n",
    "    \n",
    "for result in tqdm(\n",
    "    map(temp_f, data_files),\n",
    "    total=len(data_files),\n",
    "):\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Return Files\n",
    "\n",
    "Aggregate up return files to RV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ticker_file(filename):\n",
    "    \n",
    "    ticker = filename.split('/')[-1].split('_')[0]\n",
    "    raw_ticker_df = pd.read_feather(filename)\n",
    "    \n",
    "    return ticker, compute_rv_stats(raw_ticker_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc93c537156f4f11a7c287ac4975105e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=471.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 9.78 s, sys: 2.78 s, total: 12.6 s\n",
      "Wall time: 28.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_files = glob.glob(\"../data/returns/ticker_returns/*.feather\")\n",
    "\n",
    "with Pool(8) as p:\n",
    "\n",
    "    for ticker, processed_ticker_df in tqdm(\n",
    "        p.imap_unordered(process_ticker_file, data_files),\n",
    "        total=len(data_files),\n",
    "    ):\n",
    "\n",
    "        processed_ticker_df.to_feather(\n",
    "            f\"../data/returns/ticker_daily_rv/{ticker}_daily_rv.feather\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1447 entries, 0 to 1446\n",
      "Data columns (total 48 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   ticker              1447 non-null   object        \n",
      " 1   date                1447 non-null   datetime64[ns]\n",
      " 2   return              1447 non-null   float64       \n",
      " 3   rv                  1447 non-null   float64       \n",
      " 4   return_p4           1447 non-null   float64       \n",
      " 5   return_lag1         1447 non-null   float64       \n",
      " 6   return_bp           1447 non-null   float64       \n",
      " 7   bipower_var_daily   1447 non-null   float64       \n",
      " 8   bipower_var_tod     1447 non-null   float64       \n",
      " 9   tod_correction      1447 non-null   float64       \n",
      " 10  jump_cut            1447 non-null   float64       \n",
      " 11  is_jump             1447 non-null   int64         \n",
      " 12  rs_pos              1447 non-null   float64       \n",
      " 13  rs_neg              1447 non-null   float64       \n",
      " 14  sgn_jump_var        1447 non-null   float64       \n",
      " 15  return_d            1447 non-null   float64       \n",
      " 16  return_d_pos        1447 non-null   float64       \n",
      " 17  return_d_neg        1447 non-null   float64       \n",
      " 18  return_d_p2         1447 non-null   float64       \n",
      " 19  return_d_p4         1447 non-null   float64       \n",
      " 20  return_d_p2_pos     1447 non-null   float64       \n",
      " 21  return_d_p4_pos     1447 non-null   float64       \n",
      " 22  return_d_p2_neg     1447 non-null   float64       \n",
      " 23  return_d_p4_neg     1447 non-null   float64       \n",
      " 24  return_j            1447 non-null   float64       \n",
      " 25  return_j_pos        1447 non-null   float64       \n",
      " 26  return_j_neg        1447 non-null   float64       \n",
      " 27  return_j_p2         1447 non-null   float64       \n",
      " 28  return_j_p4         1447 non-null   float64       \n",
      " 29  return_j_p2_pos     1447 non-null   float64       \n",
      " 30  return_j_p4_pos     1447 non-null   float64       \n",
      " 31  return_j_p2_neg     1447 non-null   float64       \n",
      " 32  return_j_p4_neg     1447 non-null   float64       \n",
      " 33  rv_lag_1            1446 non-null   float64       \n",
      " 34  rv_lag_m5           1442 non-null   float64       \n",
      " 35  rv_lag_m22          1425 non-null   float64       \n",
      " 36  rv_d_1              1447 non-null   float64       \n",
      " 37  rv_d_5              1443 non-null   float64       \n",
      " 38  rv_d_22             1426 non-null   float64       \n",
      " 39  sum_jump_p2_1       1447 non-null   float64       \n",
      " 40  sum_jump_p2_5       1443 non-null   float64       \n",
      " 41  sum_jump_p2_22      1426 non-null   float64       \n",
      " 42  sum_jump_p2_pos_1   1447 non-null   float64       \n",
      " 43  sum_jump_p2_pos_5   1443 non-null   float64       \n",
      " 44  sum_jump_p2_pos_22  1426 non-null   float64       \n",
      " 45  sum_jump_p2_neg_1   1447 non-null   float64       \n",
      " 46  sum_jump_p2_neg_5   1443 non-null   float64       \n",
      " 47  sum_jump_p2_neg_22  1426 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(45), int64(1), object(1)\n",
      "memory usage: 542.8+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_feather('../data/returns/ticker_daily_rv/AAP_daily_rv.feather')\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2015-01-02T00:00:00.000000000', '2015-01-05T00:00:00.000000000',\n",
       "       '2015-01-06T00:00:00.000000000', ...,\n",
       "       '2020-09-28T00:00:00.000000000', '2020-09-29T00:00:00.000000000',\n",
       "       '2020-09-30T00:00:00.000000000'], dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['date'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AMZN Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 112866 entries, 0 to 112865\n",
      "Data columns (total 37 columns):\n",
      " #   Column             Non-Null Count   Dtype         \n",
      "---  ------             --------------   -----         \n",
      " 0   date               112866 non-null  datetime64[ns]\n",
      " 1   time               112866 non-null  object        \n",
      " 2   ticker             112866 non-null  object        \n",
      " 3   datetime           112866 non-null  datetime64[ns]\n",
      " 4   price              112866 non-null  float64       \n",
      " 5   log_price          112866 non-null  float64       \n",
      " 6   return             111419 non-null  float64       \n",
      " 7   return_p2          111419 non-null  float64       \n",
      " 8   return_p4          111419 non-null  float64       \n",
      " 9   return_lag1        109972 non-null  float64       \n",
      " 10  return_bp          109972 non-null  float64       \n",
      " 11  bipower_var_daily  112866 non-null  float64       \n",
      " 12  bipower_var_tod    109972 non-null  float64       \n",
      " 13  tod_correction     109972 non-null  float64       \n",
      " 14  jump_cut           109972 non-null  float64       \n",
      " 15  is_jump            112866 non-null  int64         \n",
      " 16  return_p2_pos      111419 non-null  float64       \n",
      " 17  return_p2_neg      111419 non-null  float64       \n",
      " 18  sgn_jump_var       111419 non-null  float64       \n",
      " 19  return_d           111419 non-null  float64       \n",
      " 20  return_d_pos       111419 non-null  float64       \n",
      " 21  return_d_neg       111419 non-null  float64       \n",
      " 22  return_d_p2        111419 non-null  float64       \n",
      " 23  return_d_p4        111419 non-null  float64       \n",
      " 24  return_d_p2_pos    111419 non-null  float64       \n",
      " 25  return_d_p4_pos    111419 non-null  float64       \n",
      " 26  return_d_p2_neg    111419 non-null  float64       \n",
      " 27  return_d_p4_neg    111419 non-null  float64       \n",
      " 28  return_j           111419 non-null  float64       \n",
      " 29  return_j_pos       111419 non-null  float64       \n",
      " 30  return_j_neg       111419 non-null  float64       \n",
      " 31  return_j_p2        111419 non-null  float64       \n",
      " 32  return_j_p4        111419 non-null  float64       \n",
      " 33  return_j_p2_pos    111419 non-null  float64       \n",
      " 34  return_j_p4_pos    111419 non-null  float64       \n",
      " 35  return_j_p2_neg    111419 non-null  float64       \n",
      " 36  return_j_p4_neg    111419 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(32), int64(1), object(2)\n",
      "memory usage: 31.9+ MB\n"
     ]
    }
   ],
   "source": [
    "pd.read_feather('../data/returns/ticker_returns/NFLX_processed.feather').info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "amzn_prices_old_df = pd.read_feather('../data/temp/AMZN_prices.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Failed to open local file '../data/returns/tickers_processed/AMZN_processed.feather'. Detail: [errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-54c3b0174480>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m amzn_prices_new_df = pd.read_feather(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m\"../data/returns/tickers_processed/AMZN_processed.feather\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/feather_format.py\u001b[0m in \u001b[0;36mread_feather\u001b[0;34m(path, columns, use_threads)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_filepath_or_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeather\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# s3fs only validates the credentials when the file is closed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyarrow/feather.py\u001b[0m in \u001b[0;36mread_feather\u001b[0;34m(source, columns, use_threads, memory_map)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \"\"\"\n\u001b[1;32m    214\u001b[0m     \u001b[0m_check_pandas_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     return (read_table(source, columns=columns, memory_map=memory_map)\n\u001b[0m\u001b[1;32m    216\u001b[0m             .to_pandas(use_threads=use_threads))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyarrow/feather.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(source, columns, memory_map)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \"\"\"\n\u001b[1;32m    236\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeatherReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_memory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyarrow/feather.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.FeatherReader.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.get_reader\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.get_native_file\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.memory_map\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.MemoryMappedFile._open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Failed to open local file '../data/returns/tickers_processed/AMZN_processed.feather'. Detail: [errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "amzn_prices_new_df = pd.read_feather(\n",
    "    \"../data/returns/tickers_processed/AMZN_processed.feather\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amzn_prices_old_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = amzn_prices_new_df.merge(amzn_prices_old_df, how = 'inner', on = ['ticker', 'date', 'datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cols = [y for y in merged_df.columns if '_x' in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for check_col  in check_cols:\n",
    "    print(check_col[:-2])\n",
    "    try: \n",
    "        print(np.max(np.abs(merged_df[check_col].astype(float) - merged_df[check_col[:-2] + '_y'].astype(float))))\n",
    "    except:\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(merged_df['price_x'] - merged_df['price_y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(merged_df['return_x'] - merged_df['return_y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(merged_df['return_x'] - merged_df['return_y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(merged_df['price_x'] - merged_df['price_y'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
